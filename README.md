# ü§ñ BERT Next Word Prediction

> **Transforming BERT into a Next-Word Oracle** üîÆ  
> Fine-tune BERT for causal language modeling and watch it predict the future of your sentences!

[![Python](https://img.shields.io/badge/Python-3.7+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.9+-red.svg)](https://pytorch.org)
[![Transformers](https://img.shields.io/badge/ü§ó%20Transformers-4.0+-yellow.svg)](https://huggingface.co/transformers)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## üåü What Makes This Special?

This project takes BERT‚Äîoriginally designed for **bidirectional** understanding‚Äîand creatively adapts it for **next word prediction**. It's like teaching a scholar who reads entire books at once to become a storyteller who builds narratives word by word!

### ‚ú® Key Features

- üéØ **Smart Masking Strategy**: Only masks the last token for focused next-word learning
- üìä **Real-time Training Monitoring**: Beautiful loss curves and accuracy tracking
- üöÄ **GPU Acceleration**: Optimized for both CPU and CUDA training
- üìà **Comprehensive Evaluation**: Top-K accuracy metrics with detailed analysis
- üé® **Professional Logging**: Training history saved as JSON with visualization
- ‚ö° **Memory Efficient**: Gradient accumulation and mixed precision training

---

## üß† The Science Behind It

### How We Transform BERT

```
Traditional BERT MLM:    "The cat [MASK] on the mat" ‚Üí "sat"
Our Approach:           "The cat sat on the [MASK]" ‚Üí "mat"
```

Instead of random masking, we strategically mask only the **last token**, turning BERT into a next-word prediction engine!

### Architecture Overview

```mermaid
graph TD
    A[Input Text] --> B[Tokenization]
    B --> C[Mask Last Token]
    C --> D[BERT Encoder]
    D --> E[MLM Head]
    E --> F[Next Word Prediction]
```

---

## üöÄ Quick Start

### Prerequisites

```bash
pip install torch transformers matplotlib numpy
```

### Basic Usage

```python
from next_bert import ImprovedBERTTrainerNextWord, NextWordEvaluator

# 1. Initialize trainer
trainer = ImprovedBERTTrainerNextWord(use_large=False)

# 2. Train on your data
trainer.train_model(
    train_file="your_data.txt",
    output_dir="./bert-nextword",
    num_epochs=5,
    batch_size=32
)

# 3. Evaluate the model
evaluator = NextWordEvaluator("./bert-nextword/final_model")
predictions = evaluator.predict_next_word("The weather is", top_k=5)
print(f"Next word predictions: {predictions}")
```

---

## üîß Configuration Options

### Training Parameters

| Parameter | Description | Default | Recommended Range |
|-----------|-------------|---------|-------------------|
| `num_epochs` | Training epochs | `5` | `3-10` |
| `batch_size` | Batch size per device | `32` | `16-64` |
| `learning_rate` | Learning rate | `2e-5` | `1e-5 to 5e-5` |
| `block_size` | Max sequence length | `256` | `128-512` |
| `validation_split` | Validation data ratio | `0.1` | `0.1-0.2` |

### Model Variants

```python
# Standard BERT (110M parameters)
trainer = ImprovedBERTTrainerNextWord(use_large=False)

# BERT-Large (340M parameters) - Better quality, slower training
trainer = ImprovedBERTTrainerNextWord(use_large=True)
```

---

## üìä Training Insights

### What You'll See During Training

- **Real-time Loss Monitoring**: Watch your model learn with decreasing loss
- **Learning Rate Scheduling**: Automatic warmup and linear decay
- **Gradient Accumulation**: Effective larger batch sizes on limited hardware
- **Mixed Precision**: Faster training with FP16 when available

### Sample Training Output

```
Using device: cuda
Loaded 50,000 lines from data.txt
Train samples: 45,000
Validation samples: 5,000
Starting training (Next Word Prediction with BERT)...
==================================================
Epoch 1/5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1406/1406 [15:32<00:00, loss=2.341]
Evaluation: eval_loss=2.156
...
Model saved to: ./bert-nextword/final_model
Training curves saved: ./bert-nextword/training_curves.png
```

---

## üéØ Performance & Evaluation

### Metrics We Track

- **Training Loss**: How well the model fits training data
- **Validation Loss**: Generalization performance
- **Top-K Accuracy**: Percentage of correct predictions in top K candidates
- **Learning Rate**: Optimizer scheduling visualization

### Example Evaluation Results

```python
evaluator = NextWordEvaluator("./bert-nextword/final_model")

test_sentences = [
    "The quick brown fox jumps over the lazy",
    "She went to the store to buy some",
    "Artificial intelligence is changing the"
]

# Top-5 accuracy: 78.3%
accuracy = evaluator.evaluate(test_sentences, top_k=5)
```

---

## üí° Advanced Usage

### Custom Data Collator

The `NextWordPredictionCollator` is the heart of our approach:

```python
class NextWordPredictionCollator:
    def __call__(self, examples):
        # Clone input as labels
        labels = input_ids.clone()
        
        # Mask all tokens except the last one
        labels[:, :-1] = self.ignore_index
        
        # Replace last token with [MASK]
        input_ids[:, -1] = self.mask_token_id
```

### Training Callbacks

Monitor training progress with custom callbacks:

```python
class HistoryCallback(TrainerCallback):
    def on_log(self, args, state, control, logs=None, **kwargs):
        # Log training metrics in real-time
        # Save loss curves and learning rates
```

---

## üé® Visualization Features

### Training Curves

The script automatically generates beautiful training visualizations:

- **Loss Curves**: Training vs. validation loss over time
- **Learning Rate Schedule**: Optimizer behavior visualization  
- **High DPI Export**: Publication-ready plots (300 DPI)

### Sample Visualization

```python
# Automatically generated after training
plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(training_history['train_loss'], label='Training Loss')
plt.plot(training_history['eval_loss'], label='Validation Loss')
```

---

## üõ†Ô∏è Technical Details

### Memory Optimization

- **Gradient Accumulation**: Simulate larger batch sizes
- **Mixed Precision (FP16)**: Reduce memory usage by ~50%
- **DataLoader Workers**: Parallel data loading
- **Pin Memory**: Faster GPU transfers

### Hardware Requirements

| Setup | Minimum | Recommended |
|-------|---------|-------------|
| **RAM** | 8GB | 16GB+ |
| **GPU VRAM** | 4GB | 8GB+ |
| **Storage** | 2GB | 5GB+ |

---

## ü§ù Contributing

We welcome contributions! Here's how you can help:

1. üêõ **Bug Reports**: Found an issue? Open an issue!
2. üí° **Feature Requests**: Have ideas? We'd love to hear them!
3. üîß **Code Contributions**: Fork, code, and submit a PR!
4. üìñ **Documentation**: Help improve our docs!

---

## üìö References & Inspiration

- [BERT: Pre-training of Deep Bidirectional Transformers](https://arxiv.org/abs/1810.04805)
- [HuggingFace Transformers Documentation](https://huggingface.co/transformers/)
- [PyTorch Lightning for Production ML](https://pytorch-lightning.readthedocs.io/)

---

## üôã‚Äç‚ôÄÔ∏è Support & Questions

- üí¨ **Issues**: Open a GitHub issue for bugs or questions
- üìß **Contact**: Reach out for collaboration opportunities
- ‚≠ê **Star**: If this project helped you, consider giving it a star!

---

<div align="center">

**Made with ‚ù§Ô∏è and lots of ‚òï**

*Happy training! May your losses be low and your predictions be accurate* üéØ

</div>
